{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "딥러닝 architecture\n",
    "corelation filter들을 묶어서 filter bank를 만들고\n",
    "그 filter bank의 Cascaded 구조가 딥러닝 구조임\n",
    "\n",
    "블랙박스가 아니다\n",
    "MFCC가 여기서 나오네.. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "어쨌든 각 레이어마다 다른 주파수의 영역을 판단한다는걸 말하고자했던거 같음 \n",
    "아래는 chatgpt에서 나온내용임\n",
    "'''\n",
    "\"Cascaded Deep Learning\"은 다단계로 연결된 심층 학습 모델을 의미합니다. 이는 여러 개의 신경망이 연속적으로 학습되어 더 복잡한 작업을 수행하는 방식을 나타냅니다.\n",
    "\n",
    "일반적으로, 첫 번째 신경망은 초기 입력 데이터를 받아 중간 수준의 특징을 추출합니다. 이 특징들은 두 번째 신경망에 전달되어 더 높은 수준의 특징을 추출합니다. 이런 식으로 여러 단계를 거쳐 더 복잡한 정보를 추출하게 됩니다.\n",
    "\n",
    "예를 들어, 얼굴 인식 시스템에서는 첫 번째 신경망이 저수준의 특징 (예: 선, 에지)을 추출하고, 두 번째 신경망은 이러한 특징을 결합하여 눈, 코, 입과 같은 고수준의 특징을 추출할 수 있습니다.\n",
    "\n",
    "Cascaded Deep Learning의 장점은 다양한 수준의 특징을 추출하고 이를 활용하여 더 정확한 예측이나 분류를 수행할 수 있다는 점입니다. 또한 각 단계에서 학습된 모델을 재사용하여 효율적으로 학습할 수 있습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "뉴런마다 인풋값이 전부다 연결되어있는것\n",
    "네트워크\n",
    "node(뉴런)-edge(커넥션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "선형대수특 값을 따로 가지고다니기 귀찮아서 한꺼번에 vector, matrix 형태로 가지고다님\n",
    "셋팅한 1,2,3...n개의 뉴런의 연산을 그대로 해줄 수 있는 구조로 만든거임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^TW+b^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias로 row vector로 만드는데, 나중에 broadcasting같은 기능 사용할때 더 편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "그리고 그냥 뉴런 하나씩 연산하고 activation scalar형태로 태우는거니까\n",
    "뉴런갯수만큼 vector가 layer의 output으로 나오겠지\n",
    "뉴련갯수 왜 헷갈리게 l로 표기하는걸까.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R^{1 \\times l}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row vector(xT)를 넣어서 row vertor(결국 각각의 activation value들)가 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "저번시간이랑 다른게 뭔지 잘 모르겠네"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "그 다음 레이어부터는 xT가 아니라 output자체가 넘어옴. l길이를 가진 row vector\n",
    "새로운 xT로 봐줘도 되겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W \\in {R^{l_{i-1} \\times l_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "결국 이전 결과 i-1번째의 뉴런길이 l_{i-1}와 현재 뉴런길이가 W의 크기를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minibatches\n",
    "이제 영민이에서.. 민수, 민지가 생김"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R^{1\\times l}$$ 을 쓴게..\n",
    "&rightarrow;\n",
    "$$R^{N\\times l}$$ 할려고였군.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix가 있을때 가로(row)는 각각 사람(N)에 대한거고 (Batch-wise)\n",
    "열은 셋팅한 과목수 or 모든 과목의 정보를 담고있는 뉴런수임 (Neuron-wise)\n",
    "\n",
    "하나의 mini batch는 하나의반임\n",
    "\n",
    "구니까 원래 구조를 떠올리려면 한사람만 떠올리면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Laysers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1, 10)\n",
      "W: (10, 3)\n",
      "B: (3,)\n",
      "Y: (1, 3)\n",
      "\n",
      "[[-0.41952312  0.4716655  -0.5002242 ]\n",
      " [ 0.5600183   0.5002588   0.10658395]\n",
      " [ 0.4985876  -0.27985284  0.27077603]\n",
      " [-0.10455358 -0.592419   -0.01186359]\n",
      " [-0.01741588  0.37600327  0.3161683 ]\n",
      " [-0.23857275 -0.56369627  0.39198697]\n",
      " [ 0.45858657  0.3405639  -0.21674198]\n",
      " [-0.03214979  0.5238031  -0.6376017 ]\n",
      " [-0.05144036  0.50637543  0.453781  ]\n",
      " [ 0.43707895 -0.07747614 -0.16472238]]\n",
      "\n",
      "tf.Tensor([[0.6508973 0.4733562 0.6782346]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature =1, 10\n",
    "X= tf.random.normal(shape=(N,n_feature))\n",
    "\n",
    "n_neuron=3\n",
    "dense=Dense(units=n_neuron, activation='sigmoid')\n",
    "Y=dense(X)\n",
    "\n",
    "W,B = dense.get_weights()\n",
    "\n",
    "print('X:',X.shape)\n",
    "print('W:',W.shape)\n",
    "print('B:',B.shape)\n",
    "print('Y:',Y.shape)\n",
    "print()\n",
    "print(W)\n",
    "print()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5, 10)\n",
      "W: (10, 3)\n",
      "B: (3,)\n",
      "Y: (5, 3)\n",
      "\n",
      "[[-0.66481113 -0.52674556 -0.61355346]\n",
      " [ 0.40042913  0.34762692 -0.35688666]\n",
      " [ 0.411991   -0.39023295 -0.20915076]\n",
      " [ 0.4533671   0.21780533 -0.12565094]\n",
      " [-0.15490568 -0.40375385  0.04237068]\n",
      " [ 0.09114134 -0.31263256 -0.4698606 ]\n",
      " [ 0.04551554 -0.22495773 -0.2946378 ]\n",
      " [-0.17980325  0.3099935   0.11367899]\n",
      " [ 0.19926697  0.6702039   0.3964895 ]\n",
      " [ 0.2754177  -0.214158   -0.65305364]]\n",
      "\n",
      "Y(tensor): tf.Tensor(\n",
      "[[0.427451   0.74169433 0.807637  ]\n",
      " [0.44275445 0.09695899 0.07993889]\n",
      " [0.4634342  0.4604019  0.52215827]\n",
      " [0.51897854 0.7522563  0.81984365]\n",
      " [0.44413015 0.4013339  0.28929877]], shape=(5, 3), dtype=float32)\n",
      "Y(man_matmul): tf.Tensor(\n",
      "[[0.427451   0.74169433 0.807637  ]\n",
      " [0.44275445 0.09695899 0.07993888]\n",
      " [0.46343422 0.46040192 0.5221583 ]\n",
      " [0.51897854 0.75225633 0.8198437 ]\n",
      " [0.44413015 0.40133393 0.2892988 ]], shape=(5, 3), dtype=float32)\n",
      "Y(y_man_vec): [[0.42745101 0.74169434 0.80763698]\n",
      " [0.44275439 0.09695896 0.07993889]\n",
      " [0.4634342  0.46040194 0.5221583 ]\n",
      " [0.51897856 0.7522563  0.81984369]\n",
      " [0.44413014 0.40133391 0.28929882]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature =5, 10\n",
    "X= tf.random.normal(shape=(N,n_feature))\n",
    "\n",
    "n_neuron=3\n",
    "dense=Dense(units=n_neuron, activation='sigmoid')\n",
    "Y=dense(X)\n",
    "\n",
    "W,B = dense.get_weights()\n",
    "\n",
    "print('X:',X.shape)\n",
    "print('W:',W.shape)\n",
    "print('B:',B.shape)\n",
    "print('Y:',Y.shape)\n",
    "print()\n",
    "print(W)\n",
    "print()\n",
    "print('Y(tensor):',Y)\n",
    "\n",
    "# calculate with matrix multiplication\n",
    "z=matmul(X,W)+B\n",
    "y_man_matmul=1/(1+exp(-z))\n",
    "print('Y(man_matmul):',y_man_matmul)\n",
    "\n",
    "# numpy로 (tensor은 역전파때매 슬라이싱 입력 불가하기 때매 numpy로)\n",
    "y_man_vec=np.zeros(shape=(N,n_neuron))\n",
    "for x_idx in range(N):\n",
    "    x=X[x_idx]\n",
    "    for nu_idx in range(n_neuron):\n",
    "        w, b=W[:, nu_idx], B[nu_idx]\n",
    "        # 이게 dot product인가 보네?\n",
    "        z=tf.reduce_sum(x*w)+b\n",
    "        a=1/(1+np.exp(-z))\n",
    "        y_man_vec[x_idx, nu_idx]=a\n",
    "        \n",
    "print('Y(y_man_vec):',y_man_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cascaded dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input X.shape (4, 10)\n",
      "next dense idx 1\n",
      "(4, 100) \n",
      "\n",
      "next dense idx 2\n",
      "(4, 20) \n",
      "\n",
      "next dense idx 3\n",
      "(4, 30) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 4, 10\n",
    "X= tf.random.normal(shape=(N, n_feature))\n",
    "X_cp = tf.identity(X) # 데이터 좌표값 다르게 카피하기 위함\n",
    "\n",
    "n_neurons = [100,20,30]\n",
    "\n",
    "dense_layers = list()\n",
    "\n",
    "for n_neuron in n_neurons:\n",
    "    dense = Dense(units=n_neuron, activation='relu')\n",
    "    dense_layers.append(dense)\n",
    "\n",
    "print('input X.shape',X.shape)\n",
    "\n",
    "for dense_idx, dense in enumerate(dense_layers):\n",
    "    X = dense(X)\n",
    "    print('next dense idx',dense_idx+1)\n",
    "    print(X.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init X.shape (4, 10)\n",
      "tensorflow result :\n",
      " tf.Tensor(\n",
      "[[0.5770336  0.6517063  0.4497505  0.3754923  0.37081444]\n",
      " [0.5785256  0.65142393 0.46340042 0.37827572 0.37299103]\n",
      " [0.5821927  0.6452699  0.4535228  0.365272   0.37594804]\n",
      " [0.57575893 0.6400241  0.40802988 0.36742997 0.37761903]], shape=(4, 5), dtype=float32)\n",
      "manual result :\n",
      " tf.Tensor(\n",
      "[[0.57703364 0.6517063  0.44975057 0.37549227 0.37081444]\n",
      " [0.57852554 0.6514239  0.46340042 0.37827566 0.37299103]\n",
      " [0.5821927  0.6452699  0.4535228  0.36527196 0.375948  ]\n",
      " [0.575759   0.6400241  0.40802985 0.36742994 0.377619  ]], shape=(4, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 4, 10\n",
    "X= tf.random.normal(shape=(N, n_feature))\n",
    "print('init X.shape',X.shape)\n",
    "X_cp = tf.identity(X) # 데이터 주소값 다르게 카피하기 위함\n",
    "\n",
    "n_neurons = [3,4,5]\n",
    "\n",
    "dense_layers = list()\n",
    "for n_neuron in n_neurons:\n",
    "    dense = Dense(units=n_neuron, activation='sigmoid')\n",
    "    dense_layers.append(dense)\n",
    "    \n",
    "# forward propagation (tensorflow)\n",
    "W, B = list(), list()\n",
    "for dense_idx, dense in enumerate(dense_layers):\n",
    "    X = dense(X)\n",
    "    w, b = dense.get_weights() ##### 사실상 여기만들기위해서 함수를만든것\n",
    "    \n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "print('tensorflow result :\\n',X) # 4개의 minibatch, 5개의 output\n",
    "\n",
    "# forward propagation (manual)\n",
    "for layer_idx in range(len(n_neurons)):\n",
    "    w,b = W[layer_idx], B[layer_idx]\n",
    "    \n",
    "    X_cp = matmul(X_cp,w) + b\n",
    "    X_cp = 1/(1+ exp(-X_cp))\n",
    "    \n",
    "print('manual result :\\n',X_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model implementation -Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# n_neurons=[3,4,5,6]\n",
    "\n",
    "# model=list()\n",
    "# for n_neuron in n_neurons:\n",
    "#     model.append(Dense(units=n_neuron, activation='sigmoid'))\n",
    "    \n",
    "# model=Sequential()\n",
    "# for n_neuron in n_neurons:\n",
    "#     model.add(Dense(units=n_neuron, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "model = Sequential() # Sequential : 모든 레이어가 연속적으로 이어져있음 (반례 residual같은 구조)\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "model.add(Dense(units=20, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model implementation with Model-subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 설계만\n",
    "class TestModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        \n",
    "        self.dense1 = Dense(units=10, activation='sigmoid')\n",
    "        self.dense2 = Dense(units=20, activation='sigmoid')\n",
    "        \n",
    "model=TestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward propagation by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential \n",
      " [[0.52097195 0.51046455 0.49297002]\n",
      " [0.5257906  0.5138255  0.49162364]\n",
      " [0.5015352  0.51093894 0.5023775 ]\n",
      " [0.48264214 0.5100943  0.5111468 ]]\n",
      "Model-subclassing \n",
      " [[0.52940357 0.46731427 0.67074335]\n",
      " [0.5904248  0.4594844  0.6780982 ]\n",
      " [0.5350664  0.4814275  0.58850074]\n",
      " [0.6183733  0.44682696 0.7267448 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "X= tf.random.normal(shape=(4,10))\n",
    "\n",
    "# Sequential\n",
    "model = Sequential() # Sequential : 모든 레이어가 연속적으로 이어져있음 (반례 residual같은 구조)\n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "model.add(Dense(units=3, activation='sigmoid'))\n",
    "Y=model(X)\n",
    "print('Sequential \\n',Y.numpy())\n",
    "\n",
    "# Model-subclassing\n",
    "class TestModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        \n",
    "        self.dense1 = Dense(units=2, activation='sigmoid')\n",
    "        self.dense2 = Dense(units=3, activation='sigmoid')\n",
    "        \n",
    "    def call(self,x): # Sequential 구조뿐 아니라 다른구조의 layer도 같이 구성가능해짐\n",
    "        x= self.dense1(x)\n",
    "        x= self.dense2(x)\n",
    "        return x      \n",
    "model = TestModel()\n",
    "Y=model(X)\n",
    "print('Model-subclassing \\n',Y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(Model):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "        self.dense_layers = list()\n",
    "        \n",
    "        for n_neuron in n_neurons:\n",
    "            self.dense_layers.append(Dense(units=n_neuron, activation='sigmoid'))\n",
    "        \n",
    "    def call(self,x): # Sequential 구조뿐 아니라 다른구조의 layer도 같이 구성가능해짐\n",
    "        for dense in self.dense_layers:\n",
    "            x=dense(x)\n",
    "        return x\n",
    "\n",
    "class TestModel(Model):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "        self.model = Sequential() # Sequential : 모든 레이어가 연속적으로 이어져있음 (반례 residual같은 구조)\n",
    "        self.model.add(Dense(units=10, activation='sigmoid'))\n",
    "        self.model.add(Dense(units=20, activation='sigmoid'))\n",
    "\n",
    "    def call(self,x): # Sequential 구조뿐 아니라 다른구조의 layer도 같이 구성가능해짐\n",
    "        x=self.model(x)\n",
    "        return x\n",
    "    \n",
    "n_neurons = [3,4,5]\n",
    "model=TestModel(n_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[<keras.layers.core.dense.Dense object at 0x7f8d50206cd0>, <keras.layers.core.dense.Dense object at 0x7f8bc0631e90>]\n"
     ]
    }
   ],
   "source": [
    "X= tf.random.normal(shape=(4,10))\n",
    "\n",
    "# Sequential\n",
    "model = Sequential() # Sequential : 모든 레이어가 연속적으로 이어져있음 (반례 residual같은 구조)\n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "model.add(Dense(units=3, activation='sigmoid'))\n",
    "Y=model(X)\n",
    "\n",
    "print(type(model.layers))\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (2,)\n",
      "(2, 3) (3,)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    w,b = layer.get_weights()\n",
    "    print(w.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_TF_MODULE_IGNORED_PROPERTIES', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_activity_regularizer', '_add_trackable', '_add_trackable_child', '_add_variable_with_custom_getter', '_auto_config', '_auto_get_config', '_auto_track_sub_layers', '_autocast', '_autographed_call', '_build_input_shape', '_call_spec', '_callable_losses', '_captured_weight_regularizer', '_cast_single_input', '_checkpoint_dependencies', '_clear_losses', '_compute_dtype', '_compute_dtype_object', '_dedup_weights', '_deferred_dependencies', '_delete_tracking', '_deserialization_dependencies', '_deserialize_from_proto', '_dtype', '_dtype_policy', '_dynamic', '_eager_losses', '_expects_mask_arg', '_expects_training_arg', '_export_to_saved_model_graph', '_flatten', '_flatten_layers', '_flatten_modules', '_functional_construction_call', '_gather_children_attribute', '_gather_saveables_for_checkpoint', '_get_cell_name', '_get_existing_metric', '_get_input_masks', '_get_node_attribute_at_index', '_get_save_spec', '_get_trainable_state', '_get_unnested_name_scope', '_handle_activity_regularization', '_handle_deferred_dependencies', '_handle_weight_regularization', '_inbound_nodes', '_inbound_nodes_value', '_infer_output_signature', '_init_call_fn_args', '_init_set_name', '_initial_weights', '_input_spec', '_instrument_layer_creation', '_instrumented_keras_api', '_instrumented_keras_layer_class', '_instrumented_keras_model_class', '_is_layer', '_keras_api_names', '_keras_api_names_v1', '_keras_tensor_symbolic_call', '_load_own_variables', '_lookup_dependency', '_losses', '_map_resources', '_maybe_build', '_maybe_cast_inputs', '_maybe_create_attribute', '_maybe_initialize_trackable', '_metrics', '_metrics_lock', '_must_restore_from_config', '_name', '_name_based_attribute_restore', '_name_based_restores', '_name_scope', '_name_scope_on_declaration', '_no_dependency', '_non_trainable_weights', '_obj_reference_counts', '_obj_reference_counts_dict', '_object_identifier', '_outbound_nodes', '_outbound_nodes_value', '_preload_simple_restoration', '_preserve_input_structure_in_config', '_restore_from_tensors', '_save_own_variables', '_saved_model_arg_spec', '_saved_model_inputs_spec', '_self_name_based_restores', '_self_saveable_object_factories', '_self_setattr_tracking', '_self_tracked_trackables', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_deferred_dependencies', '_self_unconditional_dependency_names', '_self_update_uid', '_serialize_to_proto', '_serialize_to_tensors', '_set_connectivity_metadata', '_set_dtype_policy', '_set_mask_keras_history_checked', '_set_mask_metadata', '_set_save_spec', '_set_trainable_state', '_set_training_mode', '_setattr_tracking', '_should_cast_single_input', '_stateful', '_supports_masking', '_tf_api_names', '_tf_api_names_v1', '_thread_local', '_track_trackable', '_track_variable', '_track_variables', '_trackable_children', '_trackable_saved_model_saver', '_tracking_metadata', '_trainable', '_trainable_weights', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_trackables', '_update_uid', '_updates', '_use_input_spec_as_call_signature', 'activation', 'activity_regularizer', 'add_loss', 'add_metric', 'add_update', 'add_variable', 'add_weight', 'bias', 'bias_constraint', 'bias_initializer', 'bias_regularizer', 'build', 'built', 'call', 'compute_dtype', 'compute_mask', 'compute_output_shape', 'compute_output_signature', 'count_params', 'dtype', 'dtype_policy', 'dynamic', 'finalize_state', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_weights', 'inbound_nodes', 'input', 'input_mask', 'input_shape', 'input_spec', 'kernel', 'kernel_constraint', 'kernel_initializer', 'kernel_regularizer', 'losses', 'metrics', 'name', 'name_scope', 'non_trainable_variables', 'non_trainable_weights', 'outbound_nodes', 'output', 'output_mask', 'output_shape', 'set_weights', 'stateful', 'submodules', 'supports_masking', 'trainable', 'trainable_variables', 'trainable_weights', 'units', 'updates', 'use_bias', 'variable_dtype', 'variables', 'weights', 'with_name_scope']\n"
     ]
    }
   ],
   "source": [
    "print(dir(dense1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainable_variables in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "(10, 2)\n",
      "<tf.Variable 'dense_106/kernel:0' shape=(10, 2) dtype=float32, numpy=\n",
      "array([[ 0.25764197, -0.06895328],\n",
      "       [ 0.29453248, -0.33223855],\n",
      "       [-0.6127316 , -0.07995242],\n",
      "       [ 0.7023575 ,  0.6406508 ],\n",
      "       [-0.4453826 ,  0.65480083],\n",
      "       [-0.32135606,  0.6879274 ],\n",
      "       [ 0.5607249 ,  0.22435713],\n",
      "       [ 0.03787065,  0.14771187],\n",
      "       [-0.43753013, -0.57617706],\n",
      "       [-0.43138292, -0.46534958]], dtype=float32)>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "(2,)\n",
      "<tf.Variable 'dense_106/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "(2, 3)\n",
      "<tf.Variable 'dense_107/kernel:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 0.9667678 ,  0.7102977 , -0.66608554],\n",
      "       [ 0.44596505,  0.51039946,  0.89250946]], dtype=float32)>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "(3,)\n",
      "<tf.Variable 'dense_107/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "X= tf.random.normal(shape=(4,10))\n",
    "\n",
    "# Sequential\n",
    "model = Sequential() # Sequential : 모든 레이어가 연속적으로 이어져있음 (반례 residual같은 구조)\n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "model.add(Dense(units=3, activation='sigmoid'))\n",
    "Y=model(X)\n",
    "\n",
    "print(type(model.trainable_variables))\n",
    "print(len(model.trainable_variables))\n",
    "\n",
    "for temp in model.trainable_variables: # 학습 대상이되는 variable들이 들어있음\n",
    "    print(type(temp))\n",
    "    print(temp.shape)\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
